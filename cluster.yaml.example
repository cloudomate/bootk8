# ─────────────────────────────────────────────────────────────────
# cluster.yaml — Bootstrap node cluster configuration
# Copy to cluster.yaml and fill in your values before running.
# ─────────────────────────────────────────────────────────────────

cluster:
  name: my-k8s-cluster

  # Virtual IP / load balancer IP for the control plane (HA)
  # If single control plane, use the controller-1 IP directly
  control_plane_vip: 10.0.0.10

  # Kubernetes and Flatcar versions
  k8s_version: v1.31.0
  flatcar_version: "3975.2.2"

  # Pod and service CIDRs (must not overlap with node network)
  pod_subnet: 10.244.0.0/16
  service_subnet: 10.96.0.0/12

  # SSH public key for the 'core' user on all nodes
  ssh_authorized_key: "ssh-ed25519 AAAA... your-public-key-here"

  # kubeadm bootstrap token (leave empty to auto-generate)
  # Format: [a-z0-9]{6}.[a-z0-9]{16}
  kubeadm_token: ""

# ─── Bootstrap node ───────────────────────────────────────────────
# The machine running this container image
bootstrap:
  ip: 10.0.0.5
  mac: "52:54:00:aa:bb:00"    # MAC address of the bootstrap node NIC

# ─── Control plane nodes ─────────────────────────────────────────
# 1 node = single control plane
# 3 nodes = HA (recommended for production)
controllers:
  - name: controller-1
    ip: 10.0.0.11
    mac: "52:54:00:aa:bb:01"

  - name: controller-2
    ip: 10.0.0.12
    mac: "52:54:00:aa:bb:02"

  - name: controller-3
    ip: 10.0.0.13
    mac: "52:54:00:aa:bb:03"

# ─── Worker nodes ─────────────────────────────────────────────────
workers:
  - name: worker-1
    ip: 10.0.0.21
    mac: "52:54:00:aa:bb:11"

  - name: worker-2
    ip: 10.0.0.22
    mac: "52:54:00:aa:bb:12"

  - name: worker-3
    ip: 10.0.0.23
    mac: "52:54:00:aa:bb:13"

# ─── Platform add-ons ─────────────────────────────────────────────
# All add-ons are installed automatically after the cluster is ready.
addons:

  # Flannel CNI — pod networking (required for nodes to reach Ready)
  flannel:
    enabled: true
    version: "v0.25.7"        # kube-flannel release tag

  # MetalLB — bare metal LoadBalancer for Kubernetes services
  # Nodes must be on the same L2 segment; ip_pool must not conflict with DHCP.
  metallb:
    enabled: true
    version: "v0.14.9"
    ip_pool: "10.0.0.200-10.0.0.250"   # IP range handed to LoadBalancer services

  # Cert-Manager — automated TLS certificate management
  cert_manager:
    enabled: true
    version: "v1.16.2"
    # A self-signed ClusterIssuer is created automatically.
    # Swap for ACME / Let's Encrypt by editing templates/addons/cert-manager-issuer.yaml.tmpl

  # Rook-Ceph — distributed block & file storage for bare metal
  # Each worker node needs at least one raw (unformatted) block device for OSD.
  # The rbd kernel module is automatically loaded via Ignition on all nodes.
  rook_ceph:
    enabled: true
    version: "v1.15.6"
    # Device filter applied to worker nodes — matches all raw block devices by default.
    # Set to e.g. "^sd[b-z]" to limit to specific disks.
    osd_device_filter: "^sd[b-z]|^vd[b-z]|^nvme[0-9]n[0-9]"
    replica_count: 3          # Ceph pool replica count (use 1 only for single-node testing)

  # Nebraska — self-hosted Flatcar Container Linux update server
  # Controls when and which OS version nodes upgrade to.
  # Requires MetalLB (for the LoadBalancer IP) and Longhorn (for the PostgreSQL PVC).
  nebraska:
    enabled: true
    version: "v2.8.14"
    ip: "10.0.0.200"          # Fixed IP from the MetalLB ip_pool above
    # Flatcar nodes are configured at boot to use this server for OS updates.
    # Access the Nebraska UI at http://<ip>:8000 after cluster bootstrap.
